{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4f31d5",
   "metadata": {},
   "source": [
    "# keep only 105031 rows from the dataset for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "208377e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_excel(\"Mental Health DataSet.xlsx\",nrows=105031)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e517df0",
   "metadata": {},
   "source": [
    "# remove NaN values and Encoding categorical features: Convert Gender, Country, Occupation, and Days_Indoors into numeric representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e65e562e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country\n",
      "United States     63581\n",
      "United Kingdom    16510\n",
      "Canada             6901\n",
      "Other              2810\n",
      "Australia          2675\n",
      "Ireland            1891\n",
      "Netherlands        1626\n",
      "Sweden             1294\n",
      "Germany            1023\n",
      "India               945\n",
      "New Zealand         776\n",
      "South Africa        775\n",
      "Belgium             520\n",
      "Poland              519\n",
      "France              513\n",
      "Brazil              513\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Remove rows with any NaN values\n",
    "df = df.dropna()\n",
    "df = df.drop(columns=['mental_health_interview'])\n",
    "\n",
    "# 2. Map Days_Indoors to an ordinal integer\n",
    "order = [\n",
    "    \"Go out Every day\",  # least indoor\n",
    "    \"1-14 days\",\n",
    "    \"15-30 days\",\n",
    "    \"31-60 days\",\n",
    "    \"More than 2 months\" # most indoor\n",
    "]\n",
    "mapping = {cat: i for i, cat in enumerate(order)}\n",
    "df['Days_Indoors'] = df['Days_Indoors'].map(mapping)\n",
    "\n",
    "threshold = 500  # Set your threshold\n",
    "country_counts = df['Country'].value_counts()\n",
    "rare_countries = country_counts[country_counts < threshold].index\n",
    "\n",
    "df['Country'] = df['Country'].apply(lambda x: 'Other' if x in rare_countries else x)\n",
    "print(df['Country'].value_counts())\n",
    "\n",
    "# 3. One-hot encode the remaining true categoricals\n",
    "nominals = [\n",
    "    'Gender',\n",
    "    'Country',\n",
    "    'Occupation',\n",
    "]\n",
    "\n",
    "# Only run get_dummies if ALL nominal columns are still in the DataFrame\n",
    "if set(nominals).issubset(df.columns):\n",
    "    df = pd.get_dummies(df, columns=nominals, drop_first=False, dtype=int)\n",
    "\n",
    "df.to_excel(\"cleaned_dataset.xlsx\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bd20e2",
   "metadata": {},
   "source": [
    "# Now Train/test split to prepare for model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5594009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (82297, 54)\n",
      "X_test shape:  (20575, 54)\n",
      "y_train shape: (82297,)\n",
      "y_test shape:  (20575,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = df.drop('treatment', axis=1)\n",
    "y = df['treatment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape:  {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3e307",
   "metadata": {},
   "source": [
    "# Decision Tree code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc64b5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class DecisionTreeClassifierScratch:\n",
    "    \"\"\"\n",
    "    A simple Decision Tree classifier implemented from scratch.\n",
    "    Supports 'entropy' or 'gini' as split criteria.\n",
    "    \"\"\"\n",
    "    class Node:\n",
    "        def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "            self.feature = feature\n",
    "            self.threshold = threshold\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.value = value\n",
    "\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, criterion='entropy'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / counts.sum()\n",
    "        return -np.sum(probs * np.log2(probs + 1e-9))\n",
    "\n",
    "    def _gini(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / counts.sum()\n",
    "        return 1 - np.sum(probs**2)\n",
    "\n",
    "    def _information_gain(self, y, y_left, y_right):\n",
    "        if self.criterion == 'gini':\n",
    "            loss = self._gini\n",
    "        else:\n",
    "            loss = self._entropy\n",
    "        parent_loss = loss(y)\n",
    "        n = len(y)\n",
    "        n_left, n_right = len(y_left), len(y_right)\n",
    "        child_loss = (n_left/n)*loss(y_left) + (n_right/n)*loss(y_right)\n",
    "        return parent_loss - child_loss\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_gain = 0\n",
    "        best_feat, best_thresh = None, None\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for t in thresholds:\n",
    "                left_mask = X[:, feature_idx] <= t\n",
    "                right_mask = ~left_mask\n",
    "                if left_mask.sum() < self.min_samples_split or right_mask.sum() < self.min_samples_split:\n",
    "                    continue\n",
    "                gain = self._information_gain(y, y[left_mask], y[right_mask])\n",
    "                if gain > best_gain:\n",
    "                    best_gain, best_feat, best_thresh = gain, feature_idx, t\n",
    "\n",
    "        return best_feat, best_thresh\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        return classes[np.argmax(counts)]\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        # Stopping conditions\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) \\\n",
    "           or len(np.unique(y)) == 1 \\\n",
    "           or len(y) < self.min_samples_split:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionTreeClassifierScratch.Node(value=leaf_value)\n",
    "\n",
    "        feat, thresh = self._best_split(X, y)\n",
    "        if feat is None:\n",
    "            return DecisionTreeClassifierScratch.Node(value=self._most_common_label(y))\n",
    "\n",
    "        left_mask = X[:, feat] <= thresh\n",
    "        left = self._build_tree(X[left_mask], y[left_mask], depth+1)\n",
    "        right = self._build_tree(X[~left_mask], y[~left_mask], depth+1)\n",
    "        return DecisionTreeClassifierScratch.Node(feature=feat, threshold=thresh, left=left, right=right)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build the tree using training data.\n",
    "        X: array-like of shape (n_samples, n_features)\n",
    "        y: array-like of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        self.tree = self._build_tree(X, y)\n",
    "        return self\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        X: array-like of shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91400542",
   "metadata": {},
   "source": [
    "# Metric calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2da1f2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                         Predicted 0  Predicted 1\n",
      "Actual 0 (No Treatment)         5725         3021\n",
      "Actual 1 (Treatment)            1320        10509\n",
      "\n",
      "Accuracy:  0.7890\n",
      "Precision: 0.7767\n",
      "Recall:    0.8884\n",
      "F1-Score:  0.8288\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# --- Train scratch Decision Tree ---\n",
    "tree = DecisionTreeClassifierScratch(max_depth=10, min_samples_split=20, criterion='entropy')\n",
    "tree.fit(X_train.values, y_train.values)\n",
    "\n",
    "# --- Predictions and confusion matrix ---\n",
    "y_pred = tree.predict(X_test.values)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "\n",
    "# --- Compute metrics ---\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# --- Output results ---\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pd.DataFrame(cm, index=['Actual 0 (No Treatment)', 'Actual 1 (Treatment)'], columns=['Predicted 0', 'Predicted 1']))\n",
    "print(f\"\\nAccuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97141ef3",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9918e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class DecisionTreeClassifierRF(DecisionTreeClassifierScratch):\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, criterion='entropy',\n",
    "                 max_features=None, random_state=None):\n",
    "        super().__init__(max_depth=max_depth, min_samples_split=min_samples_split, criterion=criterion)\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        # randomly select a subset of features\n",
    "        if self.max_features and self.max_features < n_features:\n",
    "            features = self.rng.choice(n_features, self.max_features, replace=False)\n",
    "        else:\n",
    "            features = np.arange(n_features)\n",
    "\n",
    "        best_gain = 0\n",
    "        best_feat, best_thresh = None, None\n",
    "        for feature_idx in features:\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for t in thresholds:\n",
    "                left_mask = X[:, feature_idx] <= t\n",
    "                right_mask = ~left_mask\n",
    "                if left_mask.sum() < self.min_samples_split or right_mask.sum() < self.min_samples_split:\n",
    "                    continue\n",
    "                gain = self._information_gain(y, y[left_mask], y[right_mask])\n",
    "                if gain > best_gain:\n",
    "                    best_gain, best_feat, best_thresh = gain, feature_idx, t\n",
    "        return best_feat, best_thresh\n",
    "\n",
    "class RandomForestClassifierScratch:\n",
    "    def __init__(self, n_estimators=10, max_depth=None, min_samples_split=2,\n",
    "                 criterion='entropy', max_features='sqrt', random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "\n",
    "    def _get_max_features(self, n_features):\n",
    "        if isinstance(self.max_features, int):\n",
    "            return self.max_features\n",
    "        if self.max_features == 'sqrt':\n",
    "            return int(np.sqrt(n_features))\n",
    "        if self.max_features == 'log2':\n",
    "            return int(np.log2(n_features))\n",
    "        return n_features\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.trees = []\n",
    "        for i in range(self.n_estimators):\n",
    "            # Bootstrap sampling\n",
    "            indices = self.rng.choice(n_samples, n_samples, replace=True)\n",
    "            X_sample, y_sample = X[indices], y[indices]\n",
    "\n",
    "            max_feats = self._get_max_features(n_features)\n",
    "            tree = DecisionTreeClassifierRF(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                criterion=self.criterion,\n",
    "                max_features=max_feats,\n",
    "                random_state=(self.random_state + i) if self.random_state is not None else None\n",
    "            )\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        # Collect predictions from each tree\n",
    "        all_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        # Majority vote\n",
    "        y_pred = []\n",
    "        for preds in all_preds.T:\n",
    "            vote = Counter(preds).most_common(1)[0][0]\n",
    "            y_pred.append(vote)\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49046f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Confusion Matrix:\n",
      "                         Predicted 0  Predicted 1\n",
      "Actual 0 (No Treatment)         5597         3149\n",
      "Actual 1 (Treatment)            1588        10241\n",
      "\n",
      "Accuracy:    0.7698\n",
      "Precision:   0.7648\n",
      "Recall:      0.8658\n",
      "F1-Score:    0.8122\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# --- Train scratch Random Forest ---\n",
    "rf = RandomForestClassifierScratch(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=2,\n",
    "    criterion='entropy',\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train.values, y_train.values)\n",
    "\n",
    "# --- Predictions and confusion matrix ---\n",
    "y_pred_rf = rf.predict(X_test.values)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "tn, fp, fn, tp = cm_rf.ravel()\n",
    "\n",
    "# --- Compute metrics ---\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "# --- Output results ---\n",
    "print(\"Random Forest Confusion Matrix:\")\n",
    "print(pd.DataFrame(cm_rf,\n",
    "                   index=['Actual 0 (No Treatment)', 'Actual 1 (Treatment)'],\n",
    "                   columns=['Predicted 0', 'Predicted 1']))\n",
    "print(f\"\\nAccuracy:    {accuracy_rf:.4f}\")\n",
    "print(f\"Precision:   {precision_rf:.4f}\")\n",
    "print(f\"Recall:      {recall_rf:.4f}\")\n",
    "print(f\"F1-Score:    {f1_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d7b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Confusion Matrix:\n",
      "[[5617 3129]\n",
      " [2660 9169]]\n",
      "Accuracy:  0.7186\n",
      "Precision: 0.7456\n",
      "Recall:    0.7751\n",
      "F1-Score:  0.7601\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# --- Decision stump as a weak learner ---\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.feature = None\n",
    "        self.threshold = None\n",
    "        self.polarity = 1\n",
    "\n",
    "    def fit(self, X, y, sample_weights):\n",
    "        X, y, w = np.array(X), np.array(y), np.array(sample_weights)\n",
    "        n_samples, n_features = X.shape\n",
    "        min_error = float('inf')\n",
    "\n",
    "        # search best feature, threshold, and polarity\n",
    "        for feature_i in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_i])\n",
    "            for t in thresholds:\n",
    "                for polarity in [1, -1]:\n",
    "                    # predictions: +1 by default\n",
    "                    preds = np.ones(n_samples)\n",
    "                    if polarity == 1:\n",
    "                        preds[X[:, feature_i] < t] = -1\n",
    "                    else:\n",
    "                        preds[X[:, feature_i] > t] = -1\n",
    "\n",
    "                    error = np.sum(w[preds != y])\n",
    "                    if error < min_error:\n",
    "                        min_error = error\n",
    "                        self.polarity = polarity\n",
    "                        self.threshold = t\n",
    "                        self.feature = feature_i\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        n_samples = X.shape[0]\n",
    "        preds = np.ones(n_samples)\n",
    "        if self.polarity == 1:\n",
    "            preds[X[:, self.feature] < self.threshold] = -1\n",
    "        else:\n",
    "            preds[X[:, self.feature] > self.threshold] = -1\n",
    "        return preds\n",
    "\n",
    "# --- AdaBoost from scratch ---\n",
    "class AdaBoostScratch:\n",
    "    def __init__(self, n_estimators=50):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learners = []\n",
    "        self.alphas = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        # convert y to {-1, +1}\n",
    "        y_signed = np.where(y == 1, 1, -1)\n",
    "        n_samples = len(y_signed)\n",
    "        # initialize sample weights\n",
    "        w = np.ones(n_samples) / n_samples\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            stump = DecisionStump().fit(X, y_signed, w)\n",
    "            preds = stump.predict(X)\n",
    "\n",
    "            # weighted error\n",
    "            err = np.clip(np.sum(w[preds != y_signed]), 1e-10, 1-1e-10)\n",
    "            alpha = 0.5 * np.log((1 - err) / err)\n",
    "\n",
    "            # update weights\n",
    "            w *= np.exp(-alpha * y_signed * preds)\n",
    "            w /= w.sum()\n",
    "\n",
    "            self.learners.append(stump)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        # weighted sum of stump predictions\n",
    "        learner_preds = np.array([alpha * learner.predict(X)\n",
    "                                   for learner, alpha in zip(self.learners, self.alphas)])\n",
    "        y_signed_pred = np.sign(np.sum(learner_preds, axis=0))\n",
    "        # map back to {0,1}\n",
    "        return np.where(y_signed_pred == 1, 1, 0)\n",
    "\n",
    "ab = AdaBoostScratch(n_estimators=50)\n",
    "ab.fit(X_train, y_train)                # use .values arrays\n",
    "y_pred_ab = ab.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "cm = confusion_matrix(y_test, y_pred_ab)\n",
    "print(\"AdaBoost Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_ab):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_ab):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_ab):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_test, y_pred_ab):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6821b95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 54)) while a minimum of 1 is required by AdaBoostClassifier.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m X_train_clean \u001b[38;5;241m=\u001b[39m X_train[mask]\n\u001b[0;32m     12\u001b[0m y_train_clean \u001b[38;5;241m=\u001b[39m y_train[mask]\n\u001b[1;32m---> 14\u001b[0m \u001b[43msk_ab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_clean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn AdaBoost F1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, f1_score(y_test, sk_ab\u001b[38;5;241m.\u001b[39mpredict(X_test)))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:130\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build a boosted classifier/regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m _raise_for_unsupported_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[1;32m--> 130\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_regressor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(\n\u001b[0;32m    142\u001b[0m     sample_weight, X, np\u001b[38;5;241m.\u001b[39mfloat64, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ensure_non_negative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    143\u001b[0m )\n\u001b[0;32m    144\u001b[0m sample_weight \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m sample_weight\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1370\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m     )\n\u001b[0;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[1;32m-> 1370\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1389\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1130\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1128\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1133\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1134\u001b[0m         )\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1137\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 54)) while a minimum of 1 is required by AdaBoostClassifier."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree     import DecisionTreeClassifier\n",
    "\n",
    "sk_ab = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),  # stump\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "mask = ~X_train.isna().any(axis=1)\n",
    "X_train_clean = X_train[mask]\n",
    "y_train_clean = y_train[mask]\n",
    "\n",
    "sk_ab.fit(X_train_clean, y_train_clean)\n",
    "print(\"sklearn AdaBoost F1:\", f1_score(y_test, sk_ab.predict(X_test)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
