{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4f31d5",
   "metadata": {},
   "source": [
    "# keep only 105031 rows from the dataset for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "208377e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_excel(\"Mental Health DataSet.xlsx\")\n",
    "df.iloc[:105031].to_excel(\"cleaned_dataset.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e517df0",
   "metadata": {},
   "source": [
    "# remove NaN values and Encoding categorical features: Convert Gender, Country, Occupation, and Days_Indoors into numeric representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e65e562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"cleaned_dataset.xlsx\")\n",
    "\n",
    "# Remove rows with any NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "df['Gender']=le.fit_transform(df['Gender'])\n",
    "df['Country']=le.fit_transform(df['Country'])\n",
    "df['Occupation']=le.fit_transform(df['Occupation'])\n",
    "df['Days_Indoors']=le.fit_transform(df['Days_Indoors'])\n",
    "\n",
    "\n",
    "df.dropna().to_excel(\"cleaned_dataset.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bd20e2",
   "metadata": {},
   "source": [
    "# Now Train/test split to prepare for model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5594009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (82297, 15)\n",
      "X_test shape:  (20575, 15)\n",
      "y_train shape: (82297,)\n",
      "y_test shape:  (20575,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = df.drop('treatment', axis=1)\n",
    "y = df['treatment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape:  {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3e307",
   "metadata": {},
   "source": [
    "# Decision Tree code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc64b5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class DecisionTreeClassifierScratch:\n",
    "    \"\"\"\n",
    "    A simple Decision Tree classifier implemented from scratch.\n",
    "    Supports 'entropy' or 'gini' as split criteria.\n",
    "    \"\"\"\n",
    "    class Node:\n",
    "        def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "            self.feature = feature\n",
    "            self.threshold = threshold\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.value = value\n",
    "\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, criterion='entropy'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / counts.sum()\n",
    "        return -np.sum(probs * np.log2(probs + 1e-9))\n",
    "\n",
    "    def _gini(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / counts.sum()\n",
    "        return 1 - np.sum(probs**2)\n",
    "\n",
    "    def _information_gain(self, y, y_left, y_right):\n",
    "        if self.criterion == 'gini':\n",
    "            loss = self._gini\n",
    "        else:\n",
    "            loss = self._entropy\n",
    "        parent_loss = loss(y)\n",
    "        n = len(y)\n",
    "        n_left, n_right = len(y_left), len(y_right)\n",
    "        child_loss = (n_left/n)*loss(y_left) + (n_right/n)*loss(y_right)\n",
    "        return parent_loss - child_loss\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_gain = 0\n",
    "        best_feat, best_thresh = None, None\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for t in thresholds:\n",
    "                left_mask = X[:, feature_idx] <= t\n",
    "                right_mask = ~left_mask\n",
    "                if left_mask.sum() < self.min_samples_split or right_mask.sum() < self.min_samples_split:\n",
    "                    continue\n",
    "                gain = self._information_gain(y, y[left_mask], y[right_mask])\n",
    "                if gain > best_gain:\n",
    "                    best_gain, best_feat, best_thresh = gain, feature_idx, t\n",
    "\n",
    "        return best_feat, best_thresh\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        return classes[np.argmax(counts)]\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        # Stopping conditions\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) \\\n",
    "           or len(np.unique(y)) == 1 \\\n",
    "           or len(y) < self.min_samples_split:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionTreeClassifierScratch.Node(value=leaf_value)\n",
    "\n",
    "        feat, thresh = self._best_split(X, y)\n",
    "        if feat is None:\n",
    "            return DecisionTreeClassifierScratch.Node(value=self._most_common_label(y))\n",
    "\n",
    "        left_mask = X[:, feat] <= thresh\n",
    "        left = self._build_tree(X[left_mask], y[left_mask], depth+1)\n",
    "        right = self._build_tree(X[~left_mask], y[~left_mask], depth+1)\n",
    "        return DecisionTreeClassifierScratch.Node(feature=feat, threshold=thresh, left=left, right=right)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build the tree using training data.\n",
    "        X: array-like of shape (n_samples, n_features)\n",
    "        y: array-like of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        self.tree = self._build_tree(X, y)\n",
    "        return self\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        X: array-like of shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91400542",
   "metadata": {},
   "source": [
    "# Metric calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2da1f2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                         Predicted 0  Predicted 1\n",
      "Actual 0 (No Treatment)         5850         2896\n",
      "Actual 1 (Treatment)            2457         9372\n",
      "\n",
      "Accuracy:  0.7398\n",
      "Precision: 0.7639\n",
      "Recall:    0.7923\n",
      "F1-Score:  0.7779\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# --- Train scratch Decision Tree ---\n",
    "tree = DecisionTreeClassifierScratch(max_depth=5, min_samples_split=10, criterion='entropy')\n",
    "tree.fit(X_train.values, y_train.values)\n",
    "\n",
    "# --- Predictions and confusion matrix ---\n",
    "y_pred = tree.predict(X_test.values)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "\n",
    "# --- Compute metrics ---\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# --- Output results ---\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pd.DataFrame(cm, index=['Actual 0 (No Treatment)', 'Actual 1 (Treatment)'], columns=['Predicted 0', 'Predicted 1']))\n",
    "print(f\"\\nAccuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97141ef3",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9918e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class DecisionTreeClassifierRF(DecisionTreeClassifierScratch):\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, criterion='entropy',\n",
    "                 max_features=None, random_state=None):\n",
    "        super().__init__(max_depth=max_depth, min_samples_split=min_samples_split, criterion=criterion)\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        # randomly select a subset of features\n",
    "        if self.max_features and self.max_features < n_features:\n",
    "            features = self.rng.choice(n_features, self.max_features, replace=False)\n",
    "        else:\n",
    "            features = np.arange(n_features)\n",
    "\n",
    "        best_gain = 0\n",
    "        best_feat, best_thresh = None, None\n",
    "        for feature_idx in features:\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for t in thresholds:\n",
    "                left_mask = X[:, feature_idx] <= t\n",
    "                right_mask = ~left_mask\n",
    "                if left_mask.sum() < self.min_samples_split or right_mask.sum() < self.min_samples_split:\n",
    "                    continue\n",
    "                gain = self._information_gain(y, y[left_mask], y[right_mask])\n",
    "                if gain > best_gain:\n",
    "                    best_gain, best_feat, best_thresh = gain, feature_idx, t\n",
    "        return best_feat, best_thresh\n",
    "\n",
    "class RandomForestClassifierScratch:\n",
    "    def __init__(self, n_estimators=10, max_depth=None, min_samples_split=2,\n",
    "                 criterion='entropy', max_features='sqrt', random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "\n",
    "    def _get_max_features(self, n_features):\n",
    "        if isinstance(self.max_features, int):\n",
    "            return self.max_features\n",
    "        if self.max_features == 'sqrt':\n",
    "            return int(np.sqrt(n_features))\n",
    "        if self.max_features == 'log2':\n",
    "            return int(np.log2(n_features))\n",
    "        return n_features\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.trees = []\n",
    "        for i in range(self.n_estimators):\n",
    "            # Bootstrap sampling\n",
    "            indices = self.rng.choice(n_samples, n_samples, replace=True)\n",
    "            X_sample, y_sample = X[indices], y[indices]\n",
    "\n",
    "            max_feats = self._get_max_features(n_features)\n",
    "            tree = DecisionTreeClassifierRF(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                criterion=self.criterion,\n",
    "                max_features=max_feats,\n",
    "                random_state=(self.random_state + i) if self.random_state is not None else None\n",
    "            )\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        # Collect predictions from each tree\n",
    "        all_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        # Majority vote\n",
    "        y_pred = []\n",
    "        for preds in all_preds.T:\n",
    "            vote = Counter(preds).most_common(1)[0][0]\n",
    "            y_pred.append(vote)\n",
    "        return np.array(y_pred)\n",
    "\n",
    "# Example usage:\n",
    "rf = RandomForestClassifierScratch(n_estimators=20, max_depth=5, min_samples_split=10, criterion='entropy', max_features='sqrt', random_state=42)\n",
    "rf.fit(X_train.values, y_train.values)\n",
    "y_pred_rf = rf.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49046f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Confusion Matrix:\n",
      "                         Predicted 0  Predicted 1\n",
      "Actual 0 (No Treatment)         5404         3342\n",
      "Actual 1 (Treatment)            2149         9680\n",
      "\n",
      "Accuracy:    0.7331\n",
      "Precision:   0.7434\n",
      "Recall:      0.8183\n",
      "F1-Score:    0.7790\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# --- Train scratch Random Forest ---\n",
    "rf = RandomForestClassifierScratch(\n",
    "    n_estimators=20,\n",
    "    max_depth=5,\n",
    "    min_samples_split=5,\n",
    "    criterion='entropy',\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train.values, y_train.values)\n",
    "\n",
    "# --- Predictions and confusion matrix ---\n",
    "y_pred_rf = rf.predict(X_test.values)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "tn, fp, fn, tp = cm_rf.ravel()\n",
    "\n",
    "# --- Compute metrics ---\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "# --- Output results ---\n",
    "print(\"Random Forest Confusion Matrix:\")\n",
    "print(pd.DataFrame(cm_rf,\n",
    "                   index=['Actual 0 (No Treatment)', 'Actual 1 (Treatment)'],\n",
    "                   columns=['Predicted 0', 'Predicted 1']))\n",
    "print(f\"\\nAccuracy:    {accuracy_rf:.4f}\")\n",
    "print(f\"Precision:   {precision_rf:.4f}\")\n",
    "print(f\"Recall:      {recall_rf:.4f}\")\n",
    "print(f\"F1-Score:    {f1_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d7b25",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m X_num \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOccupation\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDays_Indoors\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# --- One‐hot + scale numeric ---\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m ohe \u001b[38;5;241m=\u001b[39m \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfirst\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m X_cat_enc \u001b[38;5;241m=\u001b[39m ohe\u001b[38;5;241m.\u001b[39mfit_transform(X_cat)\n\u001b[0;32m     24\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "\u001b[1;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# --- Decision stump as a weak learner ---\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.feature = None\n",
    "        self.threshold = None\n",
    "        self.polarity = 1\n",
    "\n",
    "    def fit(self, X, y, sample_weights):\n",
    "        X, y, w = np.array(X), np.array(y), np.array(sample_weights)\n",
    "        n_samples, n_features = X.shape\n",
    "        min_error = float('inf')\n",
    "\n",
    "        # search best feature, threshold, and polarity\n",
    "        for feature_i in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_i])\n",
    "            for t in thresholds:\n",
    "                for polarity in [1, -1]:\n",
    "                    # predictions: +1 by default\n",
    "                    preds = np.ones(n_samples)\n",
    "                    if polarity == 1:\n",
    "                        preds[X[:, feature_i] < t] = -1\n",
    "                    else:\n",
    "                        preds[X[:, feature_i] > t] = -1\n",
    "\n",
    "                    error = np.sum(w[preds != y])\n",
    "                    if error < min_error:\n",
    "                        min_error = error\n",
    "                        self.polarity = polarity\n",
    "                        self.threshold = t\n",
    "                        self.feature = feature_i\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        n_samples = X.shape[0]\n",
    "        preds = np.ones(n_samples)\n",
    "        if self.polarity == 1:\n",
    "            preds[X[:, self.feature] < self.threshold] = -1\n",
    "        else:\n",
    "            preds[X[:, self.feature] > self.threshold] = -1\n",
    "        return preds\n",
    "\n",
    "# --- AdaBoost from scratch ---\n",
    "class AdaBoostScratch:\n",
    "    def __init__(self, n_estimators=50):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learners = []\n",
    "        self.alphas = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        # convert y to {-1, +1}\n",
    "        y_signed = np.where(y == 1, 1, -1)\n",
    "        n_samples = len(y_signed)\n",
    "        # initialize sample weights\n",
    "        w = np.ones(n_samples) / n_samples\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            stump = DecisionStump().fit(X, y_signed, w)\n",
    "            preds = stump.predict(X)\n",
    "\n",
    "            # weighted error\n",
    "            err = np.clip(np.sum(w[preds != y_signed]), 1e-10, 1-1e-10)\n",
    "            alpha = 0.5 * np.log((1 - err) / err)\n",
    "\n",
    "            # update weights\n",
    "            w *= np.exp(-alpha * y_signed * preds)\n",
    "            w /= w.sum()\n",
    "\n",
    "            self.learners.append(stump)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        # weighted sum of stump predictions\n",
    "        learner_preds = np.array([alpha * learner.predict(X)\n",
    "                                   for learner, alpha in zip(self.learners, self.alphas)])\n",
    "        y_signed_pred = np.sign(np.sum(learner_preds, axis=0))\n",
    "        # map back to {0,1}\n",
    "        return np.where(y_signed_pred == 1, 1, 0)\n",
    "\n",
    "ab = AdaBoostScratch(n_estimators=20)\n",
    "ab.fit(X_train, y_train)                # use .values arrays\n",
    "y_pred_ab = ab.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "cm = confusion_matrix(y_test, y_pred_ab)\n",
    "print(\"AdaBoost Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_ab):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_ab):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_ab):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_test, y_pred_ab):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
